{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dppRVBWdUgFj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "def clean_text(text):\n",
        "    import re\n",
        "    text = text.replace('ك', 'ک').replace('ي', 'ی').replace('ۀ', 'ه').replace('ة', 'ه')\n",
        "    text = text.replace('أ', 'ا').replace('إ', 'ا').replace('ؤ', 'و').replace('ئ', 'ی').replace('آ', 'ا')\n",
        "    text = text.replace('\\u200c', '').replace('\\u0640', '').replace('\\u200d', '').replace('\\u200e', '').replace('\\u200f', '')\n",
        "    text = re.sub(r'\\s*=\\s*', '=', text)\n",
        "    text = re.sub(r'\\s*\\+\\s*', '+', text)\n",
        "    text = re.sub(r'\\s*-\\s*', '-', text)\n",
        "    text = re.sub(r'\\s*\\*\\s*', '*', text)\n",
        "    text = re.sub(r'\\s*/\\s*', '/', text)\n",
        "    text = re.sub(r'\\s*–\\s*', '–', text)\n",
        "    text = re.sub(r'\\s*…', '...', text).replace('...', '…')\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'\\s+([.,،؛!?])', r'\\1', text)\n",
        "    text = re.sub(r'([.,،؛!?])([^\\s])', r'\\1 \\2', text)\n",
        "    text = re.sub(r'[A-Z]', lambda m: m.group(0).lower(), text)\n",
        "    return text.strip()\n",
        "\n",
        "def load_model(model_name, is_lora=False):\n",
        "    if is_lora:\n",
        "        peft_config = PeftConfig.from_pretrained(model_name)\n",
        "        base_model = AutoModelForQuestionAnswering.from_pretrained(peft_config.base_model_name_or_path)\n",
        "        model = PeftModel.from_pretrained(base_model, model_name)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path)\n",
        "    else:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "    return model, tokenizer\n",
        "\n",
        "def answer_question(model, tokenizer, question, context):\n",
        "    model.eval()\n",
        "    inputs = tokenizer(\n",
        "        question,\n",
        "        context,\n",
        "        max_length=512,\n",
        "        truncation=\"only_second\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**{k: v.to(model.device) for k, v in inputs.items()})\n",
        "        start_logits = outputs.start_logits\n",
        "        end_logits = outputs.end_logits\n",
        "\n",
        "    start_idx = torch.argmax(start_logits)\n",
        "    end_idx = torch.argmax(end_logits)\n",
        "\n",
        "    answer_tokens = inputs[\"input_ids\"][0][start_idx:end_idx+1]\n",
        "    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
        "    return clean_text(answer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Your context & question\n",
        "    context = \"\"\"کوروش بزرگ بنیان‌گذار هخامنشیان بود و در حدود ۲۵۰۰ سال پیش در ایران حکومت می‌کرد.\"\"\"\n",
        "    question = \"\"\"بنیان‌گذار هخامنشیان چه کسی بود؟\"\"\"\n",
        "\n",
        "\n",
        "    context = clean_text(context)\n",
        "    question = clean_text(question)\n",
        "\n",
        "    # Load LoRA model\n",
        "    model1, tokenizer1 = load_model(\"mmatinm/parsbert_question_answering_PersianQA_m\", is_lora=True)\n",
        "    model1.to(device)\n",
        "    answer1 = answer_question(model1, tokenizer1, question, context)\n",
        "\n",
        "    # Load normal model\n",
        "    model2, tokenizer2 = load_model(\"mmatinm/mpersian_xlm_roberta_large\", is_lora=False)\n",
        "    model2.to(device)\n",
        "    answer2 = answer_question(model2, tokenizer2, question, context)\n",
        "\n",
        "    print(\"\\nContext:\", context)\n",
        "    print(\"Question:\", question)\n",
        "    print(\"\\nBert Model Answer:\", answer1)\n",
        "    print(\"XLM-Roberta Model Answer:\", answer2)"
      ],
      "metadata": {
        "id": "Yp1Nx-trUlKv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}